{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf21eb2-215d-46ab-a1f8-121ec670f51f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.00000000e+00 4.88039915e-02 1.19807448e-01 ... 6.87900515e+02\n 6.87903010e+02 6.87905500e+02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m scaler_y \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     47\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler_X\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m---> 48\u001b[0m y_scaled \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 将数据转换为LSTM所需的时间步格式\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataset\u001b[39m(X, y, time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, forecast_horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    491\u001b[0m     X,\n\u001b[0;32m    492\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[0;32m    493\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    494\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    495\u001b[0m )\n\u001b[0;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mE:\\Netzwerk\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1049\u001b[0m             )\n\u001b[1;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.00000000e+00 4.88039915e-02 1.19807448e-01 ... 6.87900515e+02\n 6.87903010e+02 6.87905500e+02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "def merge_csv_files(directory, Import_csv_file):\n",
    "    # 存储所有DataFrame的列表\n",
    "    df_list = []\n",
    "    # 遍历目录\n",
    "    # for filename in os.listdir(directory):\n",
    "        # if filename.endswith('.csv'):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == Import_csv_file:\n",
    "                # 读取CSV文件\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                df_list.append(df)\n",
    "\n",
    "    # 合并所有DataFrame\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "# 使用函数\n",
    "directory_path = './data'  # 指定目录路径\n",
    "Import_csv_file = 'Importdata_4ms.csv'  # 合并后的Import CSV文件名\n",
    "Exoport_csv_file = 'Exportdata_4ms.csv'  # 合并后的Import CSV文件名\n",
    "Import_merged_data = merge_csv_files(directory_path, Import_csv_file)\n",
    "Exoport_merged_data = merge_csv_files(directory_path, Exoport_csv_file)\n",
    "merged_data = pd.merge(Import_merged_data, Exoport_merged_data, left_index=True, right_index=True)\n",
    "\n",
    "# 获取表头\n",
    "headers = merged_data.columns.tolist()\n",
    "\n",
    "# # 选择输入和输出列\n",
    "X = merged_data[headers[0:7]].values  # 输入列\n",
    "y = merged_data[headers[8]].values #y = merged_data[headers[8:14]].values  # 输出列\n",
    "\n",
    "# 归一化数据\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))#y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# 将数据转换为LSTM所需的时间步格式\n",
    "def create_dataset(X, y, time_step=5, forecast_horizon=1):\n",
    "    X_data, y_data = [], []\n",
    "    for i in range(len(X) - time_step - forecast_horizon + 1):\n",
    "        X_data.append(X[i:(i + time_step)])  # 过去time_step个时间步的数据\n",
    "        y_data.append(y[i + time_step:i + time_step + forecast_horizon])  # 预测未来forecast_horizon个时间步\n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "time_step = 5  # 输入时间步数\n",
    "forecast_horizon = 1  # 预测1个未来时间步\n",
    "X_data, y_data = create_dataset(X_scaled, y_scaled, time_step, forecast_horizon)\n",
    "\n",
    "# 拆分数据为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建LSTM模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    units=100,\n",
    "    return_sequences=False,\n",
    "    input_shape=(time_step, X_train.shape[2])\n",
    "))\n",
    "\n",
    "# 创建LSTM模型\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(\n",
    "#    units=100,\n",
    "#    return_sequences=True,\n",
    "#    input_shape=(time_step, X_train.shape[2])\n",
    "#))\n",
    "\n",
    "# 第二层 LSTM，不返回序列（最终输出）\n",
    "#model.add(LSTM(\n",
    "#    units=64,\n",
    "#    return_sequences=False\n",
    "#))\n",
    "\n",
    "# 输入形状调整：假设 X_train 的 shape 是 (样本数, time_step, 特征数)\n",
    "#time_step = X_train.shape[1]\n",
    "#feature_dim = X_train.shape[2]\n",
    "\n",
    "# 模型结构\n",
    "#model = Sequential()\n",
    "\n",
    "# 使用 Input 层定义输入\n",
    "#model.add(Input(shape=(time_step, feature_dim)))\n",
    "\n",
    "# 第一层 LSTM\n",
    "#model.add(LSTM(units=100, return_sequences=True))\n",
    "\n",
    "# 第二层 LSTM\n",
    "#model.add(LSTM(units=64, return_sequences=False))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(units=forecast_horizon * 1))  # 输出多个时间步#model.add(Dense(units=forecast_horizon * 6))  # 输出多个时间步\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=METRICS)\n",
    "\n",
    "# 训练模型并保存训练历史\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train.reshape(-1, forecast_horizon * 1),#6\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test.reshape(-1, forecast_horizon * 1))#6\n",
    ")\n",
    "\n",
    "model.save('./predict_model.h5')\n",
    "\n",
    "# 预测结果\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 反归一化数据\n",
    "# 由于y_pred和y_test有forecast_horizon * 6个特征，需要先重塑形状再进行反归一化\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # (samples * forecast_horizon, 6)#6\n",
    "y_test_reshaped = y_test.reshape(-1, 1)#6\n",
    "\n",
    "y_pred_rescaled = scaler_y.inverse_transform(y_pred_reshaped).reshape(y_pred.shape[0], forecast_horizon,1)#6\n",
    "y_test_rescaled = scaler_y.inverse_transform(y_test_reshaped).reshape(y_test.shape[0], forecast_horizon,1)#6\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(42, 1))  # 增加宽度和高度以适应多个子图\n",
    "feature_names = headers[8]#8-14\n",
    "\n",
    "for i in range(forecast_horizon):\n",
    "    for j in range(6):  # Iterate over each feature\n",
    "        plt.subplot(forecast_horizon, 1, i * 1 + j + 1)#6\n",
    "        plt.plot(y_test_rescaled[:, i, j], label=f'Actual {feature_names[j]} (Horizon {i+1})')\n",
    "        plt.plot(y_pred_rescaled[:, i, j], label=f'Predicted {feature_names[j]} (Horizon {i+1})')\n",
    "        plt.legend()\n",
    "        plt.title(f'Prediction vs Actual for {feature_names[j]} at Horizon {i+1}')\n",
    "        plt.xlabel('Sample')\n",
    "        plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 12))\n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric\n",
    "        plt.subplot(3,1,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim()\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim()\n",
    "        else:\n",
    "            plt.ylim()\n",
    "\n",
    "        plt.legend()\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5105ec0-ed02-445f-a8e8-01bbdcb2979a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
